# PhishGuard - 50% Implementation Report Content

## 1.1 Background

Phishing attacks represent one of the most prevalent cybersecurity threats in the modern digital landscape, targeting millions of users worldwide through deceptive websites designed to steal sensitive information such as login credentials, financial data, and personal details. These attacks have evolved significantly over the years, employing sophisticated techniques that make them increasingly difficult to detect through traditional means. Attackers often create fraudulent websites that closely mimic legitimate platforms, using similar domain names, visual designs, and URL structures to deceive unsuspecting users.

The sophistication of phishing attacks has necessitated the development of advanced detection mechanisms that can identify malicious websites in real-time, before users interact with them. Traditional blacklist-based approaches, while effective against known threats, fail to protect users from newly created phishing sites that have not yet been catalogued. This limitation has led to the exploration of heuristic-based detection systems that analyze URL characteristics and webpage content patterns to identify suspicious websites proactively.

Browser extensions have emerged as an ideal platform for implementing such protection mechanisms, as they can integrate seamlessly with users' browsing experience, providing real-time warnings and visual indicators without disrupting normal web navigation. Modern browsers, particularly Chrome with its Manifest V3 architecture, provide powerful APIs that enable extensions to monitor web navigation, analyze page content, and implement security checks at the network layer.

## 1.2 Motivation

The motivation behind developing PhishGuard stems from the critical need for a proactive, intelligent phishing detection system that combines multiple detection methodologies to provide comprehensive protection. While existing security solutions often rely on singular approaches—either blacklists, heuristics, or machine learning—each method has inherent limitations. Blacklists require constant updates and cannot detect zero-day phishing attacks, pure heuristic systems may generate false positives or miss sophisticated attacks, and standalone machine learning models may lack the contextual understanding provided by rule-based systems.

The hybrid approach adopted in this project aims to overcome these limitations by integrating URL-based heuristic analysis with machine learning-powered content analysis. This combination allows the system to leverage the speed and specificity of rule-based detection for obvious threats while utilizing the adaptive intelligence of machine learning to identify subtle patterns that may indicate phishing attempts. The system's ability to analyze both URL structure and webpage content provides a multi-dimensional view of potential threats, significantly improving detection accuracy.

Furthermore, the development of PhishGuard addresses the gap between enterprise-grade security solutions and consumer-level protection. Many existing browser security extensions either rely solely on outdated blacklists or require expensive subscriptions, leaving average users vulnerable. By implementing an open, extensible architecture that combines heuristic rules with trained machine learning models, PhishGuard aims to provide enterprise-level protection accessible to all users, with the flexibility to improve and adapt as new attack vectors emerge.

## 1.3 Aim and Objective

The primary aim of this project is to develop a comprehensive, intelligent phishing detection system that can identify suspicious and malicious websites in real-time, protecting users from potential security threats before they interact with harmful content. The system aims to achieve this through a hybrid detection approach that combines rule-based URL heuristics with machine learning-powered webpage content analysis, creating a robust, multi-layered defense mechanism.

The specific objectives of the current implementation phase include: (1) developing a Chrome extension that can classify websites into four distinct categories—safe, unknown, suspicious, and malicious—based on comprehensive analysis of URL characteristics and webpage features; (2) implementing a machine learning backend capable of extracting meaningful features from both URL structures and webpage HTML content, training a predictive model that can identify suspicious patterns with high accuracy; (3) creating a seamless user experience with intuitive visual indicators, actionable warnings, and configurable protection settings that do not disrupt normal browsing; and (4) establishing a foundation for future enhancements including backend server infrastructure, community moderation systems, and continuous model improvement through expanded datasets and advanced algorithms.

The project aims to achieve an accuracy rate above 83% in identifying suspicious websites while minimizing false positives, ensuring that legitimate websites are not incorrectly flagged. Additionally, the system is designed to be extensible, allowing for future integration with threat intelligence feeds, user reporting mechanisms, and community-driven verification systems that will enhance its effectiveness and reliability over time.

## 1.4 Report Outline

This report is structured to provide a comprehensive overview of the PhishGuard project's current implementation status, covering both the theoretical foundations and practical implementation details of the system developed to date. The document begins with an introduction that establishes the background, motivation, and objectives of the project, setting the context for the technical discussions that follow.

Section 2, "Study of the System," examines the core techniques employed in the implementation, exploring the hybrid detection methodology that combines URL heuristic analysis with machine learning approaches. This section discusses the various available techniques for phishing detection, compares their strengths and limitations, and reviews related work in the field to situate this project within the broader context of cybersecurity research and development.

Subsequent sections will detail the system architecture, implementation specifics of the Chrome extension and machine learning backend, evaluation methodologies, and findings from testing and validation efforts. The report concludes with a discussion of current limitations, future work planned for the remaining project phases, and conclusions drawn from the implementation completed thus far.

## 2.1 About the Technique

The PhishGuard system employs a hybrid detection technique that synergistically combines rule-based URL heuristic analysis with machine learning-powered webpage content examination. This two-tiered approach addresses the complementary strengths and weaknesses of each method, resulting in a more robust and accurate detection system than either approach could achieve independently.

The first tier utilizes URL heuristic analysis, which examines various structural and lexical characteristics of website URLs to identify potentially suspicious patterns. This includes detection of IP addresses used directly as domains, excessive subdomain nesting, hyphens within domain names, punycode encoding that might hide malicious characters, and the presence of suspicious keywords such as "login," "verify," "update," and "secure" within URL paths. These heuristics are based on well-documented patterns observed in phishing attacks, where attackers frequently employ domain manipulation techniques to create URLs that appear legitimate while redirecting to malicious servers.

The second tier incorporates machine learning analysis, specifically employing a Random Forest Classifier trained on a diverse dataset of over 11,000 legitimate and suspicious websites. The machine learning component analyzes 18 distinct features extracted from both URL structures and webpage HTML content, including URL length, domain characteristics, presence of login forms, password input fields, external link counts, image counts, and frequency of suspicious keywords within webpage content. This multi-dimensional feature analysis allows the model to identify subtle patterns and correlations that may not be immediately apparent through rule-based methods alone.

The integration of these techniques creates a complementary detection system where URL heuristics provide immediate, deterministic classification for obvious threats, while machine learning offers probabilistic analysis for edge cases and sophisticated attacks that may evade simple pattern matching. The system's decision-making logic allows either detection method to independently flag a website as suspicious, ensuring comprehensive coverage while maintaining high recall rates for potential threats.

## 2.2 Various Available Techniques

The field of phishing detection encompasses several distinct methodologies, each with unique advantages and limitations that influence their effectiveness in different scenarios. Understanding these various techniques is crucial for designing a comprehensive detection system and appreciating the rationale behind the hybrid approach adopted in this project.

**Blacklist-Based Detection** is among the oldest and most widely implemented methods, relying on curated databases of known malicious URLs maintained by security organizations such as Google Safe Browsing, Phishtank, and OpenPhish. This approach offers high precision for identified threats and minimal false positives, but suffers from the fundamental limitation of being reactive rather than proactive—it can only detect threats that have already been discovered and catalogued. The time delay between a phishing site's creation and its addition to blacklists leaves users vulnerable to zero-day attacks, and maintaining comprehensive, up-to-date lists requires significant resources and constant monitoring.

**Heuristic-Based Detection** employs rule-based algorithms that analyze URL structure, domain characteristics, and lexical patterns to identify suspicious indicators without requiring prior knowledge of specific malicious sites. This approach can detect previously unknown threats by recognizing common attack patterns, such as typo-squatting (using domains like "paypa1.com" instead of "paypal.com"), subdomain manipulation, and keyword stuffing. However, heuristic systems may generate false positives when legitimate websites employ similar patterns, and sophisticated attackers can potentially craft URLs that evade heuristic rules while still serving malicious content.

**Machine Learning-Based Detection** leverages artificial intelligence algorithms to learn patterns from large datasets of labeled examples, enabling the system to generalize and identify phishing characteristics beyond explicitly programmed rules. Techniques such as Random Forest, Support Vector Machines, Neural Networks, and Deep Learning have been applied to phishing detection, with features extracted from URLs, webpage content, and even behavioral patterns. Machine learning approaches can adapt to evolving attack techniques and identify subtle patterns, but they require substantial training data, may struggle with interpretability, and can be susceptible to adversarial examples designed to fool the model.

**Content-Based Analysis** examines the actual HTML, JavaScript, and visual elements of webpages to identify suspicious characteristics such as form structures, embedded scripts, redirect mechanisms, and visual similarity to legitimate sites. This approach can catch sophisticated attacks that use legitimate-looking domains but serve malicious content, but it requires fetching and parsing webpages, introducing latency and computational overhead.

**Hybrid Approaches**, such as the one implemented in PhishGuard, combine multiple techniques to leverage their complementary strengths. By integrating URL heuristics with machine learning content analysis, hybrid systems can achieve both the speed of rule-based detection and the adaptability of learned models, providing comprehensive protection while mitigating individual method limitations.

## 2.3 Related Works

The problem of phishing detection has been extensively researched in both academic and industry contexts, with numerous approaches and systems developed to address various aspects of the threat landscape. Understanding related work helps contextualize the PhishGuard implementation and identify both the contributions of this project and areas for future enhancement.

**Academic Research** in phishing detection has explored various machine learning algorithms and feature sets. Studies have investigated the effectiveness of different classifiers, with Random Forest, Support Vector Machines, and Neural Networks showing promise for binary classification tasks. Research has also examined optimal feature selection, identifying that combinations of URL-based features (length, subdomain count, special characters) and content-based features (form presence, external links, keyword frequency) provide superior accuracy compared to either category alone. However, many academic approaches focus on offline analysis rather than real-time browser integration, limiting their practical applicability.

**Industry Solutions** include browser-integrated systems such as Google Safe Browsing, which uses a combination of blacklists and heuristics to warn users about potentially dangerous sites. Microsoft's SmartScreen and Firefox's built-in protection systems employ similar approaches, primarily relying on reputation-based blacklists with supplementary heuristic checks. While effective, these systems are proprietary, limiting transparency and customization, and may not provide the granular control desired for specialized use cases or research applications.

**Browser Extension Solutions** such as Netcraft's Anti-Phishing Toolbar, Web of Trust (WOT), and various academic research prototypes have demonstrated the feasibility of client-side phishing detection. These solutions vary widely in their approaches—some rely primarily on community reporting and reputation systems, while others implement technical analysis similar to PhishGuard's methodology. However, many existing extensions face challenges with accuracy, user experience, or scalability, and few successfully integrate both heuristic and machine learning components in a seamless, real-time detection framework.

**Recent Developments** in the field have increasingly focused on deep learning techniques, using Convolutional Neural Networks for visual similarity detection and Natural Language Processing for analyzing webpage text content. While promising, these approaches require significant computational resources and may not be suitable for real-time browser extension deployment without cloud backend support.

The PhishGuard project contributes to this landscape by demonstrating a practical, integrated approach that combines URL heuristics with machine learning content analysis within a lightweight browser extension framework, providing real-time protection without requiring cloud dependencies for basic functionality. The system's four-label classification scheme (safe, unknown, suspicious, malicious) offers more granular threat assessment than binary approaches, enabling users to make informed decisions about website trustworthiness.

## 3.1 Problem Statement

The proliferation of phishing attacks presents a significant and growing cybersecurity challenge, with attackers continuously evolving their techniques to bypass traditional detection mechanisms. The core problem addressed by this project is the inadequacy of existing protection systems that rely on single-method approaches, which are either too reactive (blacklists) or too prone to false positives (pure heuristics), or too resource-intensive (standalone machine learning models) for real-time browser-based deployment.

Current solutions suffer from several critical limitations: blacklist-based systems cannot protect against zero-day phishing attacks until threats are identified and catalogued, creating a window of vulnerability that attackers exploit. Heuristic-based systems, while faster, often generate false alarms when legitimate websites employ similar structural patterns to phishing sites, leading to user frustration and potential bypassing of security warnings. Standalone machine learning implementations typically require cloud connectivity, raising privacy concerns and introducing latency that degrades user experience.

Additionally, the existing landscape lacks a unified system that provides granular threat assessment—most solutions offer binary safe/unsafe classifications that do not account for the spectrum of risk levels that websites may exhibit. Users require actionable information that distinguishes between confirmed malicious sites, suspicious sites requiring caution, and unknown sites that have not yet been evaluated, enabling informed decision-making rather than blanket blocking or ignoring.

The PhishGuard project addresses these problems by proposing an integrated, hybrid detection system that combines the speed of heuristic analysis with the intelligence of machine learning, operating locally within a browser extension to provide real-time, privacy-preserving protection that adapts to evolving attack patterns without requiring constant cloud connectivity for basic functionality.

## 3.2 Scope

The scope of the PhishGuard project encompasses the development of a comprehensive phishing detection system implemented as a Chrome browser extension, integrated with a machine learning backend for enhanced threat identification. The current implementation phase focuses on establishing core functionality through hybrid detection methodologies, setting the foundation for future enhancements that will expand system capabilities and effectiveness.

**In-Scope Components** include: (1) development of a Chrome extension using Manifest V3 architecture that monitors web navigation and classifies websites into four distinct categories—safe, unknown, suspicious, and malicious—based on URL characteristics and webpage content analysis; (2) implementation of a rule-based URL heuristic engine that detects suspicious patterns including IP addresses, excessive subdomains, hyphens, punycode encoding, and keyword manipulation; (3) development of a machine learning model using Random Forest classification, trained on datasets of legitimate and phishing websites, capable of analyzing 18 distinct features extracted from both URLs and webpage HTML content; (4) creation of a Python Flask API backend that serves machine learning predictions to the extension in real-time; (5) design and implementation of user interface components including dynamic toolbar icons, popup warnings, full-screen interstitials for malicious sites, and configuration options for whitelist/blacklist management; and (6) integration of dark/light theme support and user preference management using browser storage APIs.

**Out-of-Scope Elements** for the current phase include: (1) cloud-based backend infrastructure deployment, which will be addressed in future phases; (2) community moderation and user reporting systems, planned for implementation in subsequent development cycles; (3) integration with external threat intelligence feeds such as Phishtank or Google Safe Browsing APIs, scheduled for Phase 4; (4) advanced machine learning techniques such as deep learning or neural networks, which may be explored in future optimization phases; and (5) multi-browser support beyond Chrome, though the architecture is designed to be extensible to other browsers.

The project scope is intentionally focused on establishing a functional, accurate, and user-friendly core system that demonstrates the viability of hybrid detection approaches, with clear pathways for expansion and enhancement in subsequent development phases.

## 3.3 Proposed System

The PhishGuard system is proposed as an intelligent, multi-layered phishing detection solution that operates seamlessly within the Chrome browser environment, providing real-time protection through a hybrid methodology that integrates rule-based URL heuristic analysis with machine learning-powered webpage content examination. The system architecture is designed to be lightweight, privacy-preserving, and extensible, enabling continuous improvement through model retraining and feature enhancement without disrupting user experience.

The proposed system consists of three primary components: (1) a Chrome extension frontend that handles user interface presentation, browser event monitoring, and local classification using URL heuristics; (2) a machine learning backend implemented as a Python Flask API server that provides probabilistic threat assessment based on comprehensive feature analysis of webpage content; and (3) a classification engine that synthesizes inputs from both heuristic rules and machine learning predictions to assign websites to one of four risk categories, with corresponding visual indicators and user warnings.

The detection workflow begins when a user navigates to a website, triggering the extension's background service worker to extract the URL and perform initial heuristic analysis. This rapid, deterministic evaluation can immediately classify websites as safe (if present in a curated whitelist of known legitimate domains) or identify obvious suspicious patterns. For websites classified as unknown or suspicious by heuristic rules, the system queries the machine learning API, which fetches the webpage content, extracts 18 features encompassing URL structure and HTML characteristics, and generates a probabilistic prediction regarding the site's maliciousness.

The proposed system's decision-making logic employs a complementary approach where either detection method can independently flag a website as suspicious—URL heuristics provide immediate classification for obvious threats, while machine learning offers nuanced analysis for edge cases. This dual-layer protection ensures comprehensive coverage: websites with suspicious URL patterns are flagged even if webpage content appears benign, and sophisticated attacks using legitimate-looking URLs are caught through content analysis. The four-label classification scheme (safe, unknown, suspicious, malicious) provides users with actionable information, distinguishing between confirmed threats requiring blocking, suspicious sites warranting caution, and unknown sites requiring further evaluation.

User interaction is facilitated through intuitive visual indicators: dynamic toolbar icons change color based on classification (green for safe, yellow for suspicious, red for malicious, gray for unknown), popup interfaces provide detailed threat information and action options, and full-screen interstitials block access to confirmed malicious sites with options to proceed, go back, or close the tab. The system maintains user preferences including whitelist/blacklist configurations and theme settings, ensuring personalized protection that respects user autonomy while providing robust security guidance.

## 4.1 Requirement Engineering

Requirement engineering for the PhishGuard project involved systematic identification, analysis, and documentation of functional and non-functional requirements essential for delivering an effective phishing detection system. This process ensured that the system addresses real-world security needs while maintaining usability, performance, and extensibility required for practical deployment and future enhancement.

### 4.1.1 Requirement Elicitation

Requirement elicitation was conducted through analysis of existing phishing detection solutions, review of cybersecurity research literature, and identification of gaps in current protection mechanisms. The process identified key stakeholder needs including: end-users requiring real-time, non-intrusive protection that does not disrupt normal browsing activities; security researchers needing transparent, extensible systems for threat analysis; and system administrators requiring lightweight, privacy-preserving solutions that do not introduce significant performance overhead.

Functional requirements derived from this analysis include: the system must classify websites into four categories (safe, unknown, suspicious, malicious) based on URL and webpage characteristics; it must provide visual indicators (toolbar icons) that update dynamically based on classification results; it must display warning interfaces for suspicious and malicious sites with actionable options; it must support user-configurable whitelists and blacklists; it must integrate machine learning predictions for enhanced accuracy; and it must operate in real-time without noticeable latency affecting page load times.

Non-functional requirements identified encompass: performance requirements specifying that URL heuristic classification must complete within milliseconds, and machine learning API calls should not exceed 2-3 seconds; usability requirements mandating intuitive interfaces that do not require technical expertise; privacy requirements ensuring that webpage content analysis occurs locally or through user-controlled endpoints; reliability requirements specifying graceful degradation when ML backend is unavailable; and extensibility requirements enabling future integration with threat intelligence feeds and community moderation systems.

### 4.1.2 Software Lifecycle Model

The PhishGuard project adopts an iterative, incremental software development lifecycle model, specifically utilizing aspects of the Agile methodology combined with elements of the Spiral model to accommodate both rapid prototyping of core features and systematic risk management for security-critical components. This approach allows for continuous refinement based on testing feedback while maintaining structured phases for requirement validation, architecture design, implementation, and evaluation.

The current implementation represents the first major increment, focusing on establishing core functionality including URL heuristic classification, basic machine learning integration, and user interface components. Subsequent increments will introduce backend server infrastructure, community moderation capabilities, and advanced model optimization. Each increment follows a cycle of planning, design, implementation, testing, and evaluation, with user feedback informing priorities for the next iteration.

Risk-driven elements from the Spiral model are incorporated through systematic evaluation of security implications at each phase—ensuring that machine learning model accuracy is validated before deployment, that API security measures are implemented to prevent abuse, and that user privacy is protected throughout the system architecture. This hybrid lifecycle approach balances the flexibility needed for rapid development with the rigor required for security-critical applications.

### 4.1.3 Requirement Analysis

Requirement analysis involved detailed examination of elicited requirements to ensure feasibility, consistency, and traceability, identifying dependencies between functional components and establishing verification criteria for system validation.

#### 4.1.3.1 UML Diagrams/DFDs Based on the Project

The system architecture can be represented through multiple diagrammatic models: **Use Case Diagrams** illustrate interactions between actors (users, browsers, ML backend) and system components (classification engine, UI components, API server), showing use cases such as "Classify Website," "Display Warning," "Manage Whitelist," and "Query ML API." **Sequence Diagrams** depict the flow of classification requests from browser navigation events through heuristic analysis, ML API calls, and UI updates, demonstrating temporal interactions between background service worker, content scripts, popup interface, and Flask backend.

**Data Flow Diagrams (DFDs)** model information flow through the system: Level 0 DFD shows external entities (User, Web Browser, ML Backend) exchanging data with the PhishGuard system; Level 1 DFD decomposes the system into processes including "Monitor Navigation," "Extract URL Features," "Perform Heuristic Analysis," "Query ML Backend," "Synthesize Classification," "Update UI," and "Manage User Preferences," with data stores for whitelists, blacklists, user settings, and training data. **Component Diagrams** represent the structural organization showing relationships between Chrome extension modules (background.js, popup.js, content scripts), classifier components, ML backend modules (Flask app, feature extraction, model inference), and external dependencies.

#### 4.1.3.2 Cost Analysis

Cost analysis for the PhishGuard project encompasses development costs, operational expenses, and infrastructure requirements. **Development Costs** include time investment for implementation (estimated at 200-300 hours for current phase), tooling and software licenses (Chrome Web Store developer account: $5 one-time fee; development tools: free/open-source), and training data acquisition (public datasets: free; potential commercial datasets: variable). **Operational Costs** for the current local deployment model are minimal—users run the ML backend on their own machines, requiring only standard hardware resources.

**Future Infrastructure Costs** (for planned cloud deployment) include cloud hosting services (estimated $20-50/month for small-scale deployment on platforms like Heroku, AWS, or Google Cloud), database storage for threat intelligence feeds (estimated $10-30/month), and API usage fees if integrating with commercial threat intelligence services (varies by provider and usage volume). **Maintenance Costs** involve ongoing model retraining with updated datasets (estimated 10-20 hours monthly), bug fixes and feature enhancements (estimated 5-10 hours monthly), and monitoring and optimization of ML model performance.

The total cost for the current 50% implementation phase is primarily development time, with minimal monetary expenses. Future phases will require cloud infrastructure investment, but the modular architecture allows for gradual scaling based on user adoption and resource availability.

#### 4.1.3.3 Hardware and Software Requirements

**Hardware Requirements** for running PhishGuard are minimal, designed to function on standard consumer hardware: users require a computer with at least 2GB RAM (4GB recommended for optimal ML backend performance), a modern multi-core processor (2+ cores recommended for parallel data processing during model training), and standard network connectivity for fetching webpage content and, optionally, communicating with cloud-based threat intelligence feeds. Storage requirements are modest: approximately 50-100MB for the Chrome extension files, 200-500MB for Python ML backend dependencies, and 100-500MB for training datasets and model files.

**Software Requirements** include: Chrome browser version 88 or higher (supporting Manifest V3), Python 3.8 or higher for ML backend execution, and standard Python packages including Flask, scikit-learn, pandas, numpy, BeautifulSoup4, and requests. Development requirements include a code editor (VS Code, Sublime Text, or similar), Git for version control, and Chrome Developer Tools for extension debugging. The system is designed to be cross-platform, compatible with Windows, macOS, and Linux operating systems.

**Browser API Requirements** encompass Chrome extension APIs including: `chrome.tabs` for tab monitoring and URL extraction, `chrome.storage` for local data persistence, `chrome.scripting` for content script injection, and `chrome.webNavigation` for navigation event tracking. The extension requires host permissions for `<all_urls>` to enable comprehensive website analysis, and storage permissions for maintaining user preferences and classification history.

## 4.2 System Architecture

The PhishGuard system architecture follows a modular, distributed design that separates concerns between client-side browser extension components and server-side machine learning processing, enabling efficient real-time threat detection while maintaining privacy and performance objectives. The architecture is structured to support both local ML backend operation (current implementation) and future cloud-based deployment, ensuring flexibility and scalability.

### 4.2.1 UI/UX Diagram

The user interface architecture is organized into distinct components that provide seamless interaction across different contexts: **Toolbar Icon** serves as the primary visual indicator, dynamically updating color (green/yellow/red/gray) based on website classification, and providing access to detailed information through click interaction. **Popup Interface** displays comprehensive site classification details when users click the toolbar icon, showing current website status, classification reasons, threat level indicators, and actionable buttons including "Proceed Anyway," "Go Back," "Report Site," and "Close Tab" (context-dependent based on threat level).

**Full-Screen Interstitial** appears for confirmed malicious sites, blocking page access entirely and presenting a prominent warning message with options to navigate away, proceed at user's own risk, or close the tab. **Suspicious Site Banner** provides a non-intrusive, dismissible notification for suspicious sites, allowing users to acknowledge the warning while maintaining page accessibility. **Options Page** enables users to manage whitelisted and blacklisted domains, configure theme preferences (dark/light mode), and view extension settings.

The UI/UX design prioritizes information hierarchy: critical threats (malicious) receive full-screen blocking with prominent warnings, moderate threats (suspicious) receive visible but non-blocking notifications, and safe/unknown classifications provide subtle visual feedback without interrupting browsing. Color coding follows intuitive conventions: green for safe, yellow for caution (suspicious), red for danger (malicious), and gray for unclassified (unknown). The interface maintains consistency across light and dark themes, with CSS variables enabling seamless theme switching that syncs across all UI components including content script overlays and popup interfaces.

User experience flow follows a progressive disclosure model: toolbar icons provide immediate visual feedback, popup interfaces offer detailed information on demand, and interstitials provide comprehensive warnings only when necessary. This design minimizes cognitive load while ensuring users receive appropriate security guidance based on threat severity, balancing protection effectiveness with browsing convenience.

